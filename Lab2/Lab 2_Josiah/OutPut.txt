This article will quickly introduce three commonly used regression models using R and the Boston housing data-set: Ridge, Lasso, and Elastic Net.Simple linear regression, also known as ordinary least squares (OLS) attempts to minimize the sum of error squared.Visualization of the squared error (from Setosa.io)
The equation for this model is referred to as the cost function and is a way to find the optimal error by minimizing and measuring it.Equation for least ordinary squares
One situation is the data showing multi-collinearity, this is when predictor variables are correlated to each other and to the response variable.To produce a more accurate model of complex data we can add a penalty term to the OLS equation.These are known as L1 regularization(Lasso regression) and L2 regularization(ridge regression).The best model we can hope to come up with minimizes both the bias and the variance:
Ridge regression uses L2 regularization which adds the following penalty term to the OLS equation.L2 regularization penalty term
The L2 term is equal to the square of the magnitude of the coefficients.is zero then the equation is the basic OLS but if it is greater than zero then we add a constraint to the coefficients.This constraint results in minimized coefficients (aka shrinkage) that trend towards zero the larger the value of lambda.The penalty applied for L2 is equal to the absolute value of the magnitude of the coefficients:
L1 regularization penalty term
Similar to ridge regression, a lambda value of zero spits out the basic OLS equation, however given a suitable lambda value lasso regression can drive some coefficients to zero.A third commonly used model of regression is the Elastic Net which incorporates penalties from both L1 and L2 regularization:
In addition to setting and choosing a lambda value elastic net also allows us to tune the alpha parameter where ??Simply put, if you plug in 0 for alpha, the penalty function reduces to the L1 (ridge) term and if we set alpha to 1 we get the L2 (lasso) term.